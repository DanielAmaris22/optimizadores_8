{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee62491-fd89-414d-acee-aeb14b909394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El punto mínimo aproximado es: [1.11072073 1.11072073]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import numpy as np\n",
    "\n",
    "# Definir la función de pérdida\n",
    "def loss_func(theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    return -np.sin(R)\n",
    "\n",
    "# Definir el gradiente de la función de pérdida\n",
    "def evaluate_gradient(loss_func, x_train, y_train, theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    grad_x = -np.cos(R) * (x / R)\n",
    "    grad_y = -np.cos(R) * (y / R)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# Gradiente descendente\n",
    "def gd(theta, x_train, y_train, loss_func, epochs, eta):\n",
    "    for i in range(epochs):\n",
    "        gradient = evaluate_gradient(loss_func, x_train, y_train, theta)\n",
    "        theta -= eta * gradient\n",
    "        # theta = theta - eta * gradient\n",
    "    return theta, gradient\n",
    "\n",
    "# Parámetros iniciales\n",
    "theta_init = np.array([2.0, 2.0])  # Puedes elegir cualquier punto inicial\n",
    "eta = 0.1  # Tasa de aprendizaje\n",
    "epochs = 10000  # Número de iteraciones\n",
    "\n",
    "# Ejecutar gradiente descendente\n",
    "theta_final, gradient_final = gd(theta_init, None, None, loss_func, epochs, eta)\n",
    "print(f\"El punto mínimo aproximado es: {theta_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1836a0a4-9019-40fb-bf3d-5aac33d68fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El punto mínimo aproximado es: [818.02419645 574.35032942]\n"
     ]
    }
   ],
   "source": [
    "# Definir la función de pérdida\n",
    "def loss_func(theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    return -np.sin(R)\n",
    "\n",
    "# Definir el gradiente de la función de pérdida\n",
    "def evaluate_gradient(loss_func, x, y, theta):\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    grad_x = -np.cos(R) * (x / R)\n",
    "    grad_y = -np.cos(R) * (y / R)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# Gradiente descendente estocástico\n",
    "def sgd(theta, data_train, loss_func, epochs, eta):\n",
    "    for i in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos en cada época\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            gradient = evaluate_gradient(loss_func, x, y, theta)\n",
    "            theta = theta - eta * gradient  # Actualizar los parámetros con el gradiente\n",
    "    return theta, gradient\n",
    "\n",
    "# Generar datos de entrenamiento (pueden ser puntos aleatorios en el plano)\n",
    "n_points = 100\n",
    "x_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "y_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "data_train = list(zip(x_train, y_train))  # Crear pares de datos (x, y)\n",
    "\n",
    "# Parámetros iniciales\n",
    "theta_init = np.array([2.0, 2.0])  # Punto inicial\n",
    "eta = 0.01  # Tasa de aprendizaje inicial\n",
    "epochs = 10000  # Número de épocas\n",
    "\n",
    "# Ejecutar el SGD\n",
    "theta_final, gradient_final = sgd(theta_init, data_train, loss_func, epochs, eta)\n",
    "print(f\"El punto mínimo aproximado es: {theta_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ac43141-6ca9-4735-809d-ee43d0739b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El punto mínimo aproximado es: [ 134.35457131 -198.87527003]\n"
     ]
    }
   ],
   "source": [
    "# Definir la función de pérdida\n",
    "def loss_func(theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    return -np.sin(R)\n",
    "\n",
    "# Definir el gradiente de la función de pérdida\n",
    "def evaluate_gradient(loss_func, x, y, theta):\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    grad_x = -np.cos(R) * (x / R)\n",
    "    grad_y = -np.cos(R) * (y / R)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# RMSprop\n",
    "def rmsprop(theta, data_train, loss_func, epochs, eta=0.001, decay=0.9, epsilon=1e-8):\n",
    "    E_g2 = np.zeros_like(theta)  # Inicializar E[g^2] en cero\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            gradient = evaluate_gradient(loss_func, x, y, theta)\n",
    "            \n",
    "            # Actualizar el promedio del cuadrado del gradiente\n",
    "            E_g2 = decay * E_g2 + (1 - decay) * gradient**2\n",
    "            \n",
    "            # Actualizar los parámetros usando RMSprop\n",
    "            theta -= eta / (np.sqrt(E_g2) + epsilon) * gradient\n",
    "            \n",
    "    return theta\n",
    "\n",
    "# Generar datos de entrenamiento (pueden ser puntos aleatorios en el plano)\n",
    "n_points = 100\n",
    "x_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "y_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "data_train = list(zip(x_train, y_train))  # Crear pares de datos (x, y)\n",
    "\n",
    "# Parámetros iniciales\n",
    "theta_init = np.array([2.0, 2.0])  # Punto inicial\n",
    "epochs = 10000  # Número de épocas\n",
    "\n",
    "# Ejecutar RMSprop\n",
    "theta_final = rmsprop(theta_init, data_train, loss_func, epochs)\n",
    "print(f\"El punto mínimo aproximado es: {theta_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e053e51a-19f7-4518-b682-810af2d46500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El punto mínimo aproximado es: [104.72907941  10.62512374]\n"
     ]
    }
   ],
   "source": [
    "# Definir la función de pérdida\n",
    "def loss_func(theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    return -np.sin(R)\n",
    "\n",
    "# Definir el gradiente de la función de pérdida\n",
    "def evaluate_gradient(loss_func, x, y, theta):\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    grad_x = -np.cos(R) * (x / R)\n",
    "    grad_y = -np.cos(R) * (y / R)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# Algoritmo Adam\n",
    "def adam(theta, data_train, loss_func, epochs, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    m = np.zeros_like(theta)  # Inicializar el momento de primer orden\n",
    "    v = np.zeros_like(theta)  # Inicializar el momento de segundo orden\n",
    "    t = 0  # Inicializar el contador de iteraciones\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            t += 1  # Incrementar el contador\n",
    "            gradient = evaluate_gradient(loss_func, x, y, theta)\n",
    "\n",
    "            # Actualizar los momentos de primer y segundo orden\n",
    "            m = beta1 * m + (1 - beta1) * gradient\n",
    "            v = beta2 * v + (1 - beta2) * (gradient**2)\n",
    "\n",
    "            # Corrección de sesgo para momentos de primer y segundo orden\n",
    "            m_hat = m / (1 - beta1**t)\n",
    "            v_hat = v / (1 - beta2**t)\n",
    "\n",
    "            # Actualización de los parámetros\n",
    "            theta -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Generar datos de entrenamiento (pueden ser puntos aleatorios en el plano)\n",
    "n_points = 100\n",
    "x_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "y_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "data_train = list(zip(x_train, y_train))  # Crear pares de datos (x, y)\n",
    "\n",
    "# Parámetros iniciales\n",
    "theta_init = np.array([2.0, 2.0])  # Punto inicial\n",
    "epochs = 10000  # Número de épocas\n",
    "\n",
    "# Ejecutar Adam\n",
    "theta_final = adam(theta_init, data_train, loss_func, epochs)\n",
    "print(f\"El punto mínimo aproximado es: {theta_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ddff949-a3f4-49f4-8259-733846810f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Método  Distancia al origen\n",
      "0  Gradient Descent             1.570796\n",
      "1               SGD           673.212525\n",
      "2           RMSProp           129.494524\n",
      "3              Adam           132.366176\n",
      "El mejor método es: Gradient Descent con una distancia de 1.5708\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import numpy as np\n",
    "\n",
    "# Definir la función de pérdida\n",
    "def loss_func(theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    return -np.sin(R)\n",
    "\n",
    "# Definir el gradiente de la función de pérdida\n",
    "def evaluate_gradient(loss_func, theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    grad_x = -np.cos(R) * (x / R)\n",
    "    grad_y = -np.cos(R) * (y / R)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# Gradiente descendente\n",
    "def gd(theta, loss_func, epochs, eta):\n",
    "    for i in range(epochs):\n",
    "        gradient = evaluate_gradient(loss_func, theta)\n",
    "        theta -= eta * gradient\n",
    "    return theta, gradient\n",
    "\n",
    "# Gradiente descendente estocástico\n",
    "def sgd(theta, data_train, loss_func, epochs, eta):\n",
    "    for i in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos en cada época\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            gradient = evaluate_gradient(loss_func, np.array([x, y]))\n",
    "            theta -= eta * gradient  # Actualizar los parámetros con el gradiente\n",
    "    return theta, gradient\n",
    "\n",
    "# RMSprop\n",
    "def rmsprop(theta, data_train, loss_func, epochs, eta=0.001, decay=0.9, epsilon=1e-8):\n",
    "    E_g2 = np.zeros_like(theta)  # Inicializar E[g^2] en cero\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            gradient = evaluate_gradient(loss_func, np.array([x, y]))\n",
    "            # Actualizar el promedio del cuadrado del gradiente\n",
    "            E_g2 = decay * E_g2 + (1 - decay) * gradient**2\n",
    "            # Actualizar los parámetros usando RMSprop\n",
    "            theta -= eta / (np.sqrt(E_g2) + epsilon) * gradient\n",
    "    return theta\n",
    "\n",
    "# Algoritmo Adam\n",
    "def adam(theta, data_train, loss_func, epochs, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    m = np.zeros_like(theta)  # Inicializar el momento de primer orden\n",
    "    v = np.zeros_like(theta)  # Inicializar el momento de segundo orden\n",
    "    t = 0  # Inicializar el contador de iteraciones\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            t += 1  # Incrementar el contador\n",
    "            gradient = evaluate_gradient(loss_func, np.array([x, y]))\n",
    "            # Actualizar los momentos de primer y segundo orden\n",
    "            m = beta1 * m + (1 - beta1) * gradient\n",
    "            v = beta2 * v + (1 - beta2) * (gradient**2)\n",
    "            # Corrección de sesgo para momentos de primer y segundo orden\n",
    "            m_hat = m / (1 - beta1**t)\n",
    "            v_hat = v / (1 - beta2**t)\n",
    "            # Actualización de los parámetros\n",
    "            theta -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    return theta\n",
    "\n",
    "# Distancia desde el origen\n",
    "def calcular_distancia(theta):\n",
    "    return np.sqrt(theta[0]**2 + theta[1]**2)\n",
    "\n",
    "# Generar datos de entrenamiento \n",
    "n_points = 100\n",
    "x_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "y_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "data_train = list(zip(x_train, y_train))  # Crear pares de datos (x, y)\n",
    "\n",
    "# Parámetros iniciales\n",
    "theta_init = np.array([2.0, 2.0])\n",
    "epochs = 10000  \n",
    "\n",
    "# Ejecución de metodos\n",
    "\n",
    "# Gradient Descent\n",
    "theta_gd, _ = gd(theta_init.copy(), loss_func, epochs, 0.1)\n",
    "dist_gd = calcular_distancia(theta_gd)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "theta_sgd, _ = sgd(theta_init.copy(), data_train, loss_func, epochs, 0.01)\n",
    "dist_sgd = calcular_distancia(theta_sgd)\n",
    "\n",
    "# RMSProp\n",
    "theta_rmsprop = rmsprop(theta_init.copy(), data_train, loss_func, epochs)\n",
    "dist_rmsprop = calcular_distancia(theta_rmsprop)\n",
    "\n",
    "# Adam\n",
    "theta_adam = adam(theta_init.copy(), data_train, loss_func, epochs)\n",
    "dist_adam = calcular_distancia(theta_adam)\n",
    "\n",
    "# Crear la tabla de resultados\n",
    "resultados = {\n",
    "    \"Método\": [\"Gradient Descent\", \"SGD\", \"RMSProp\", \"Adam\"],\n",
    "    \"Distancia al origen\": [dist_gd, dist_sgd, dist_rmsprop, dist_adam]\n",
    "}\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Determinar el mejor método (menor distancia)\n",
    "mejor_metodo = df_resultados.loc[df_resultados['Distancia al origen'].idxmin()]\n",
    "\n",
    "# Mostrar resultados\n",
    "print(df_resultados)\n",
    "print(f\"El mejor método es: {mejor_metodo['Método']} con una distancia de {mejor_metodo['Distancia al origen']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b4c9c-2c49-419c-91b6-e6386ad0d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de pérdida\n",
    "def loss_func(theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    return -np.sin(R)\n",
    "\n",
    "# Definir el gradiente de la función de pérdida\n",
    "def evaluate_gradient(loss_func, theta):\n",
    "    x, y = theta\n",
    "    R = np.sqrt(x**2 + y**2)\n",
    "    grad_x = -np.cos(R) * (x / R)\n",
    "    grad_y = -np.cos(R) * (y / R)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# Gradiente descendente\n",
    "def gd(theta, loss_func, epochs, eta):\n",
    "    for i in range(epochs):\n",
    "        gradient = evaluate_gradient(loss_func, theta)\n",
    "        theta -= eta * gradient\n",
    "    return theta, gradient\n",
    "\n",
    "# Gradiente descendente estocástico\n",
    "def sgd(theta, data_train, loss_func, epochs, eta):\n",
    "    for i in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos en cada época\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            gradient = evaluate_gradient(loss_func, np.array([x, y]))\n",
    "            theta -= eta * gradient  # Actualizar los parámetros con el gradiente\n",
    "    return theta, gradient\n",
    "\n",
    "# RMSprop\n",
    "def rmsprop(theta, data_train, loss_func, epochs, eta=0.001, decay=0.9, epsilon=1e-8):\n",
    "    E_g2 = np.zeros_like(theta)  # Inicializar E[g^2] en cero\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            gradient = evaluate_gradient(loss_func, np.array([x, y]))\n",
    "            # Actualizar el promedio del cuadrado del gradiente\n",
    "            E_g2 = decay * E_g2 + (1 - decay) * gradient**2\n",
    "            # Actualizar los parámetros usando RMSprop\n",
    "            theta -= eta / (np.sqrt(E_g2) + epsilon) * gradient\n",
    "    return theta\n",
    "\n",
    "# Algoritmo Adam\n",
    "def adam(theta, data_train, loss_func, epochs, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    m = np.zeros_like(theta)  # Inicializar el momento de primer orden\n",
    "    v = np.zeros_like(theta)  # Inicializar el momento de segundo orden\n",
    "    t = 0  # Inicializar el contador de iteraciones\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(data_train)  # Barajar los datos\n",
    "        for example in data_train:\n",
    "            x, y = example\n",
    "            t += 1  # Incrementar el contador\n",
    "            gradient = evaluate_gradient(loss_func, np.array([x, y]))\n",
    "            # Actualizar los momentos de primer y segundo orden\n",
    "            m = beta1 * m + (1 - beta1) * gradient\n",
    "            v = beta2 * v + (1 - beta2) * (gradient**2)\n",
    "            # Corrección de sesgo para momentos de primer y segundo orden\n",
    "            m_hat = m / (1 - beta1**t)\n",
    "            v_hat = v / (1 - beta2**t)\n",
    "            # Actualización de los parámetros\n",
    "            theta -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    return theta\n",
    "\n",
    "# Distancia desde el origen\n",
    "def calcular_distancia(theta):\n",
    "    return np.sqrt(theta[0]**2 + theta[1]**2)\n",
    "\n",
    "# Generar datos de entrenamiento \n",
    "n_points = 100\n",
    "x_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "y_train = np.random.uniform(-6.5, 6.5, n_points)\n",
    "data_train = list(zip(x_train, y_train))  # Crear pares de datos (x, y)\n",
    "\n",
    "# Parámetros iniciales\n",
    "theta_init = np.array([2.0, 2.0])\n",
    "epochs = 100\n",
    "\n",
    "n_simulaciones = 50  # Número de simulaciones\n",
    "resultados_simulaciones = []\n",
    "\n",
    "for _ in range(n_simulaciones):\n",
    "    # Ejecutar cada método\n",
    "    theta_gd, _ = gd(theta_init.copy(), loss_func, epochs, 0.1)\n",
    "    dist_gd = calcular_distancia(theta_gd)\n",
    "\n",
    "    theta_sgd, _ = sgd(theta_init.copy(), data_train, loss_func, epochs, 0.01)\n",
    "    dist_sgd = calcular_distancia(theta_sgd)\n",
    "\n",
    "    theta_rmsprop = rmsprop(theta_init.copy(), data_train, loss_func, epochs)\n",
    "    dist_rmsprop = calcular_distancia(theta_rmsprop)\n",
    "\n",
    "    theta_adam = adam(theta_init.copy(), data_train, loss_func, epochs)\n",
    "    dist_adam = calcular_distancia(theta_adam)\n",
    "\n",
    "    # Guardar resultados en cada simulación\n",
    "    distancias = [dist_gd, dist_sgd, dist_rmsprop, dist_adam]\n",
    "    mejor_metodo_idx = np.argmin(distancias)\n",
    "    resultados_simulaciones.append(mejor_metodo_idx)\n",
    "\n",
    "df_frecuencias = pd.DataFrame(resultados_simulaciones, columns=[\"Mejor Método\"])\n",
    "\n",
    "df_frecuencias[\"Mejor Método\"] = df_frecuencias[\"Mejor Método\"].map({\n",
    "    0: \"Gradient Descent\",\n",
    "    1: \"SGD\",\n",
    "    2: \"RMSProp\",\n",
    "    3: \"Adam\"\n",
    "})\n",
    "\n",
    "# Tabla de frecuencias absolutas y relativas\n",
    "tabla_frecuencias = df_frecuencias[\"Mejor Método\"].value_counts().reset_index()\n",
    "tabla_frecuencias.columns = [\"Método\", \"Frecuencia Absoluta\"]\n",
    "tabla_frecuencias[\"Frecuencia Relativa\"] = tabla_frecuencias[\"Frecuencia Absoluta\"] / n_simulaciones\n",
    "\n",
    "print(tabla_frecuencias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
